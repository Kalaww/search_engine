{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import search_engine.util as util\n",
    "import re\n",
    "import os\n",
    "import pandas\n",
    "import re\n",
    "import sys\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WIKI_DIR = 'data/wiki'\n",
    "\n",
    "WIKI_DUMP_FILENAME = os.path.join(WIKI_DIR, 'frwiki-20151226-pages-articles.xml')\n",
    "WIKI_SMALL_FILENAME = os.path.join(WIKI_DIR, 'frwiki-small.xml')\n",
    "\n",
    "WIKI_N_LINES = 246871655\n",
    "\n",
    "PAGE_TO_ID_FILENAME = os.path.join(WIKI_DIR, 'page_to_id.csv')\n",
    "ID_TO_PAGE_FILENAME = os.path.join(WIKI_DIR, 'id_to_page.csv')\n",
    "PAGE_WORDS_FILENAME = os.path.join(WIKI_DIR, 'title_words.csv')\n",
    "PAGE_LINKS_FILENAME = os.path.join(WIKI_DIR, 'page_links.csv')\n",
    "WORDS_APPEARANCE_FILENAME = os.path.join(WIKI_DIR, 'words_appearance.csv')\n",
    "\n",
    "DICTIONNARY_FILENAME = 'data/dictionnary.fr.csv'\n",
    "STOPWORDS_FILENAME = 'data/stopwords.fr.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(WIKI_DIR):\n",
    "    os.mkdir(WIKI_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to print progress during file browsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pretty_number(n):\n",
    "    a = str(n)[::-1]\n",
    "    return (' '.join([a[i:i+3] for i in range(0, len(a), 3)]))[::-1]\n",
    "\n",
    "def print_progress(current_line):\n",
    "    global last_line, last_print, WIKI_N_LINES\n",
    "    if current_line - last_line < 100000:\n",
    "        return\n",
    "    sys.stdout.write('\\b' * len(last_print))\n",
    "    sys.stdout.write('\\r')\n",
    "    s = '{} / {}'.format(pretty_number(current_line), pretty_number(WIKI_N_LINES))\n",
    "    sys.stdout.write(s)\n",
    "    sys.stdout.flush()\n",
    "    last_line = current_line\n",
    "    last_print = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv(DICTIONNARY_FILENAME)\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "for i in range(len(dataframe)):\n",
    "    word_to_id[dataframe['word'][i]] = dataframe['id'][i]\n",
    "    id_to_word[dataframe['id'][i]] = dataframe['word'][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get title of each page and attach an id to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_to_id = {} # page title => [ids]\n",
    "id_to_page = {} # id => page title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd_wiki_dump = open(WIKI_SMALL_FILENAME, 'r')\n",
    "\n",
    "current_title = None\n",
    "\n",
    "re_title = '.*<title>(.+)</title>.*'\n",
    "\n",
    "last_print = ''\n",
    "last_line = 0\n",
    "line_count = 0\n",
    "\n",
    "page_id = 1\n",
    "\n",
    "for line in fd_wiki_dump:\n",
    "    line = line[:-1]\n",
    "    line = line.strip()\n",
    "    line = util.process_word(line.decode('utf-8'))\n",
    "    if '<title' in line:\n",
    "        match = re.match(re_title, line)\n",
    "        if match is None or ':' in match.group(1)\n",
    "            continue\n",
    "        current_title = match.group(1)   \n",
    "        current_title = util.normalize_page_content(current_title)\n",
    "        if current_title in page_to_id:\n",
    "            page_to_id[current_title].append(page_id)\n",
    "        else:\n",
    "            page_to_id[current_title] = [page_id]\n",
    "        id_to_page[page_id] = current_title\n",
    "        page_id += 1\n",
    "        current_title = None\n",
    "    print_progress(line_count)\n",
    "    line_count += 1\n",
    "\n",
    "fd_wiki_dump.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb titles: 380\n"
     ]
    }
   ],
   "source": [
    "print('nb titles:', pretty_number(len(id_to_page)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: extract links from page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_links(page_id, words):\n",
    "    global page_to_id\n",
    "    re_link = '\\[\\[(.*?)\\]\\]'\n",
    "    match = re.findall(re_link, words)\n",
    "    links = []\n",
    "    if match:\n",
    "        for m in match:\n",
    "            for m1 in m.split('|'):\n",
    "                if ':' in m1:\n",
    "                    continue\n",
    "                m1 = util.normalize_page_content(m1)\n",
    "                if m1 in page_to_id:\n",
    "                    links = links + page_to_id[m1]\n",
    "    return sorted(set(links) - set([page_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: extract words id from page content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_words_id(words):\n",
    "    global word_to_id\n",
    "    words = util.normalize_page_content(words)\n",
    "    words_id = [word_to_id[w] for w in words.split(' ') if w in word_to_id]\n",
    "    return words_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: append page data to word_appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_to_words_appearance(words_appearance, page_id, title, words_id):\n",
    "    title_words_id = set(word_to_id[w] for w in title.split(' ') if w in word_to_id)\n",
    "    freq_unique = 1.0 / float(len(words_id))\n",
    "    occ = dict()\n",
    "    for word_id in words_id:\n",
    "        if word_id in occ:\n",
    "            occ[word_id] = occ[word_id] + freq_unique\n",
    "        else:\n",
    "            occ[word_id] = freq_unique\n",
    "    for word_id in title_words_id:\n",
    "        if word_id in occ:\n",
    "            occ[word_id] = 1.0 + occ[word_id]\n",
    "        else:\n",
    "            occ[word_id] = 1.0\n",
    "    for k,v in occ.items():\n",
    "        if k in words_appearance:\n",
    "            words_appearance[k].append((page_id,v))\n",
    "        else:\n",
    "            words_appearance[k] = [(page_id,v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k,v in page_to_id.items():\n",
    "    page_to_id[k] = sorted(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_page = False\n",
    "in_text = False\n",
    "words = None\n",
    "page_title = None\n",
    "\n",
    "re_id = '.*<id>(\\d+)</id>.*'\n",
    "re_text_start = '^.*<text.*>(.+)$'\n",
    "re_text_end = '^(.*)</text>'\n",
    "\n",
    "last_print = ''\n",
    "last_line = 0\n",
    "line_count = 0\n",
    "\n",
    "page_to_id_cpy = copy.deepcopy(page_to_id)\n",
    "\n",
    "words_appearance = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fd_wiki_dump = open(WIKI_SMALL_FILENAME, 'r')\n",
    "\n",
    "fd_links = open(os.path.join(PAGE_LINKS_FILENAME), 'w')\n",
    "fd_links.write('page_id,links\\n')\n",
    "\n",
    "for line in fd_wiki_dump:\n",
    "    line = line[:-1]\n",
    "    line = line.strip()\n",
    "    line = util.process_word(line.decode('utf-8'))\n",
    "    if in_page:\n",
    "        if in_text:\n",
    "            if '</text' in line:\n",
    "                match = re.match(re_text_end, line)\n",
    "                if match is None:\n",
    "                    continue\n",
    "                words += ' ' + match.group(1)\n",
    "                in_text = False\n",
    "            else:\n",
    "                words += ' ' + line\n",
    "        else:\n",
    "            if '</page' in line:\n",
    "                if page_title and words:\n",
    "                    page_id = page_to_id_cpy[page_title][0]\n",
    "                    page_to_id_cpy[page_title] = page_to_id_cpy[page_title][1:]\n",
    "                    \n",
    "                    links = extract_links(page_id, words)                    \n",
    "                    fd_links.write('{},{}\\n'.format(page_id, ' '.join([str(k) for k in links])))\n",
    "                    \n",
    "                    words_id = extract_words_id(words)\n",
    "                    if len(words_id) > 0:\n",
    "                        append_to_words_appearance(\n",
    "                            words_appearance,\n",
    "                            page_id,\n",
    "                            page_title,\n",
    "                            words_id\n",
    "                        )\n",
    "                words = None\n",
    "                page_title = None\n",
    "                in_page = False\n",
    "            elif '<title' in line:\n",
    "                match = re.match(re_title, line)\n",
    "                if match is None:\n",
    "                    continue\n",
    "                page_title = match.group(1)\n",
    "                page_title = util.normalize_page_content(page_title)\n",
    "            elif '<text' in line:\n",
    "                match = re.match(re_text_start, line)\n",
    "                if match is None:\n",
    "                    continue\n",
    "                words = match.group(1)\n",
    "                in_text = True\n",
    "    else:\n",
    "        if '<page' in line:\n",
    "            in_page = True\n",
    "    print_progress(line_count)\n",
    "    line_count += 1\n",
    "\n",
    "fd_links.close()\n",
    "fd_wiki_dump.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort both page_to_id and id_to_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_to_id = sorted(page_to_id.items())\n",
    "id_to_page = sorted(id_to_page.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort words_appearance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_appearance = sorted(words_appearance.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save page_to_id in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(PAGE_TO_ID_FILENAME, 'w') as fd:\n",
    "    fd.write('page@ids\\n')\n",
    "    for page,list_id in page_to_id:\n",
    "        fd.write('{}@{}\\n'.format(page, ' '.join([str(k) for k in list_id])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save id_to_page in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(ID_TO_PAGE_FILENAME, 'w') as fd:\n",
    "    fd.write('id@page\\n')\n",
    "    for page_id,page in id_to_page:\n",
    "        fd.write('{}@{}\\n'.format(page_id, page))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save words_appearance in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(WORDS_APPEARANCE_FILENAME, 'w') as fd:\n",
    "    fd.write('word_id,frequencies\\n')\n",
    "    for word_id,freqs in words_appearance:\n",
    "        fd.write('{},{}\\n'.format(word_id, ' '.join([str(a)+':'+str(b) for a,b in freqs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
